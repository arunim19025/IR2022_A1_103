{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccaf6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7599b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importDocument(path):\n",
    "    content = {}\n",
    "    file_list = []\n",
    "    for info in os.walk(path):\n",
    "        filenames = info[2]\n",
    "        for file in filenames:\n",
    "            try:\n",
    "                with open(path+file) as f:\n",
    "                    lines = f.readlines()\n",
    "                    content[file] = lines\n",
    "                    file_list.append(file)\n",
    "            except:\n",
    "                print(\"Discarded file : \\t\",file)\n",
    "    return content, file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a77320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyWords(documents):\n",
    "    for key, value in documents.items():\n",
    "        buff = []\n",
    "        for line in range(len(value)):\n",
    "            if len(value[line].strip()) != 0:\n",
    "                linetoken = nltk.RegexpTokenizer(r\"\\w+\").tokenize(value[line])\n",
    "                linetoken = [i.lower() for i in linetoken]\n",
    "                if len(linetoken) != 0:\n",
    "                    buff.append(linetoken)\n",
    "        documents[key] = buff\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85217372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(documents):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for key, value in documents.items():\n",
    "        for line in range(len(value)):\n",
    "            value[line] = [i for i in value[line] if not i in stop_words]\n",
    "        documents[key] = value\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20b3e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(documents):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for key, value in documents.items():\n",
    "        for line in range(len(value)):\n",
    "            value[line] = [lemmatizer.lemmatize(i) for i in value[line]]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa566b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueWords(documents):\n",
    "    unique = []\n",
    "    content = {}\n",
    "    for i, t in enumerate(documents.items()):\n",
    "        value = t[1]\n",
    "        doc = []\n",
    "        for line in value:\n",
    "            doc += line\n",
    "        doc = list(set(doc))\n",
    "        unique += doc\n",
    "        for word in doc:\n",
    "            if word not in content:\n",
    "                content[word] = [1,[i+1]]\n",
    "            else:\n",
    "                content[word][0] += 1\n",
    "                content[word][1].append(i+1)\n",
    "    return list(set(unique)), content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb9e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnderscore(document, wordslist):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    remove = []\n",
    "    no_exist = []\n",
    "    for key in document:\n",
    "        if not key.isalnum():\n",
    "            word = key.replace(\"_\",\" \").strip()\n",
    "            word = nltk.RegexpTokenizer(r\"\\w+\").tokenize(word)\n",
    "            word = [i.lower() for i in word]\n",
    "            word = [lemmatizer.lemmatize(i) for i in word]\n",
    "            \n",
    "            for token in word:\n",
    "                if len(token) == 0 or token in stop_words:\n",
    "                    continue\n",
    "                elif token in wordslist:\n",
    "                    document[token][0] += document[key][0]\n",
    "                    document[token][1] =  sorted(list(set(document[token][1]+document[key][1])))\n",
    "                else:\n",
    "                    no_exist.append([token,document[key][0], document[key][1]]) \n",
    "            remove.append(key)\n",
    "    \n",
    "    \n",
    "    for item in no_exist:\n",
    "        if item[0] not in document:\n",
    "            document[item[0]] = [item[1], item[2]]\n",
    "            wordslist.append(item[0])\n",
    "        else:\n",
    "            document[token][0] += document[key][0]\n",
    "            document[token][1] =  sorted(list(set(document[token][1]+document[key][1])))\n",
    "        \n",
    "    for key in remove:\n",
    "        del document[key]\n",
    "        wordslist.remove(key)\n",
    "    \n",
    "    return wordslist, document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a97f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryPreprocess(query):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    linetoken = nltk.RegexpTokenizer(r\"\\w+\").tokenize(query)\n",
    "    linetoken = [i.lower() for i in linetoken]\n",
    "    linetoken = [i for i in linetoken if not i in stop_words]\n",
    "    linetoken = [lemmatizer.lemmatize(i) for i in linetoken]\n",
    "    return linetoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926a3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryNOT(querylist, size):\n",
    "    doclist = [0,[]]\n",
    "    for i in range(size):\n",
    "        if (i+1) not in querylist[1]:\n",
    "            doclist[0] += 1\n",
    "            doclist[1].append(i+1)\n",
    "    return doclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c7e084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryOperation(doclist, word, document, query,size, NOT = False):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    common = [0,[]]\n",
    "    union = [0,[]] \n",
    "    if not NOT:\n",
    "        querylist = document[word]\n",
    "    else:\n",
    "        querylist = queryNOT(document[word],size)\n",
    "    \n",
    "#     print(doclist[0])\n",
    "#     print(document[word][0])\n",
    "#     print(querylist[0])\n",
    "    comp = 0\n",
    "    while i<doclist[0] and j<querylist[0]:\n",
    "        comp += 1\n",
    "        if doclist[1][i] < querylist[1][j]:\n",
    "            union[1].append(doclist[1][i])\n",
    "            i += 1\n",
    "        elif doclist[1][i] > querylist[1][j]:\n",
    "            union[1].append(querylist[1][j])\n",
    "            j += 1\n",
    "        else:\n",
    "            union[1].append(doclist[1][i])\n",
    "            common[1].append(doclist[1][i])\n",
    "            common[0] += 1\n",
    "            i += 1\n",
    "            j += 1\n",
    "        union[0] += 1\n",
    "    \n",
    "    while i<doclist[0]:\n",
    "        union[1].append(doclist[1][i])\n",
    "        union[0] += 1\n",
    "        i += 1\n",
    "    \n",
    "    while j<querylist[0]:\n",
    "        union[1].append(querylist[1][j])\n",
    "        union[0] += 1\n",
    "        j += 1\n",
    "    \n",
    "#     print(union[0])\n",
    "#     print(common[0])\n",
    "    if query == \"OR\":\n",
    "        return union, comp\n",
    "    else:\n",
    "        return common, comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "badb66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(query, operation,unique_words_document,size):\n",
    "    query = queryPreprocess(query)\n",
    "    doclist = unique_words_document[query[0]]\n",
    "    comparisons = 0\n",
    "    for i in range(len(query)-1):\n",
    "        op = operation[i].split(\" \")\n",
    "        if len(op) == 1:\n",
    "            doclist, k = queryOperation(doclist,query[i+1],unique_words_document, op[0], size)\n",
    "        else:\n",
    "            doclist, k = queryOperation(doclist,query[i+1],unique_words_document, op[0], size, True)\n",
    "        comparisons += k\n",
    "        \n",
    "    print(\"Number of documents : \",doclist[0])\n",
    "    print(\"Number of comparisons : \",comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39aab747",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded file : \t hilbilly.wri\n",
      "Discarded file : \t howlong.hum\n",
      "Discarded file : \t oxymoron.txt\n",
      "Discarded file : \t steroid.txt\n",
      "Discarded file : \t various.txt\n",
      "1128\n"
     ]
    }
   ],
   "source": [
    "path = \"dataset/Humor,Hist,Media,Food/\"\n",
    "documents, files = importDocument(path)\n",
    "size = len(documents)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c6f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = onlyWords(documents)\n",
    "documents = removeStopWords(documents)\n",
    "documents = lemmatization(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56afd082",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words, unique_words_document = uniqueWords(documents)\n",
    "unique_words, unique_words_document = removeUnderscore(unique_words_document, unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e24602fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"lion stood thoughtfully for a moment\"\n",
    "# query = \"telephone,paved, roads\"\n",
    "# operation = [ \"OR\", \"OR\", \"OR\" ]\n",
    "# operation = [\"OR NOT\", \"AND NOT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc3977d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Enter query : lion stood thoughtfully for a moment\n",
      "Enter the operations : OR,OR,OR\n",
      "\n",
      "Number of documents :  210\n",
      "Number of comparisons :  399\n",
      "\n",
      "Enter query : telephone,paved, roads\n",
      "Enter the operations : OR NOT,AND NOT \n",
      "\n",
      "Number of documents :  992\n",
      "Number of comparisons :  2241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = int(input())\n",
    "for i in range(n):\n",
    "    query = str(input(\"Enter query : \"))\n",
    "    operation = list(map(str,input(\"Enter the operations : \").strip().split(',')))\n",
    "    print()\n",
    "    compute(query, operation, unique_words_document, size)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
